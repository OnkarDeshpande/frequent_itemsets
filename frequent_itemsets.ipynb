{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Algorithm implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source:\n",
    "http://fimi.ua.ac.be/data/\n",
    "    \n",
    "Data set used here: \n",
    "\n",
    "     Given by Ferenc Bodon, contains (anonymized) click-stream data of a hungarian on-line news portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'kosarak.dat.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset:\n",
    "By default only, 10K rows read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read into list of sets\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename, no_lines = 10000):\n",
    "    \"\"\"\n",
    "    Input is a space separated text filename\n",
    "    Function outputs list of sets\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    count = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            l.append(set([int(ele) for ele in line.replace('\\n','').split()]))\n",
    "            count+=1\n",
    "            if count > no_lines:\n",
    "                print('Data read into list of sets')\n",
    "                break\n",
    "    \n",
    "    return l\n",
    "\n",
    "\n",
    "basket_data = load_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basket_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Algorithm:\n",
    "\n",
    "Aim: Find frequent Itemsets (Frequent subsets with one or more items)\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "1) Generate list of items with frequency above the minimum threshold (# times a basket contains the item) \n",
    "\n",
    "2) Based on this subset, look for subsets of basket with size = 2, whose frequency crosses the threshold \n",
    "\n",
    "3) Continue this until you get a subset that don't fit the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"apriori.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note is the ... property. Which states that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prod_list(data):\n",
    "    \"\"\"\n",
    "    Initial pass through the data, to check the number of unique items\n",
    "    Returns a list of items stored as frozensets\n",
    "    Frozen set is used to avoid mutability and also as these groups are revisited again.\n",
    "    \"\"\"\n",
    "    prod_list = []\n",
    "    prod_set = set()\n",
    "    for basket in data:\n",
    "        for prod in basket:\n",
    "            if prod not in prod_set:\n",
    "                prod_set.add(prod)\n",
    "                prod_list.append([prod])\n",
    "    print('Product list created!')\n",
    "    return list(map(frozenset,prod_list))\n",
    "\n",
    "# c1 = create_prod_list(basket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lk(dataset, ck, threshold=0.01*10000):\n",
    "    \"\"\"\n",
    "    Checks for every element in ck, if a subset of basket, keep a count.\n",
    "    Passes on the sets which have count over threshold\n",
    "    \"\"\"\n",
    "    freq_count = {}\n",
    "    for basket in dataset:\n",
    "        for pair in ck:\n",
    "            if pair.issubset(basket):\n",
    "                if pair in freq_count:\n",
    "                    freq_count[pair]+=1\n",
    "                else:\n",
    "                    freq_count[pair]=1\n",
    "    \n",
    "    data_size = len(dataset)\n",
    "    sort_count = sorted(freq_count.items(), key=lambda x:x[1], reverse = True)\n",
    "    lk = []\n",
    "    support_dict = {}\n",
    "    for ele in sort_count:\n",
    "        if ele[1] >= threshold:\n",
    "            lk.append(ele[0])\n",
    "            support_dict[ele[0]] = ele[1]/data_size\n",
    "#     print(\"Created L set\")\n",
    "    lk.sort()\n",
    "    return lk, dict(support_dict)\n",
    "    \n",
    "# l1, supp_dict = create_lk(basket_data, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ck(lk,k):\n",
    "    \"\"\"\n",
    "    two sets from L that differ in just one element will be joined together.\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    lenlk = len(lk)\n",
    "    for i in range(lenlk):\n",
    "        for j in range(i+1, lenlk): \n",
    "            tmp_l1 = list(lk[i])[:k-2]\n",
    "            tmp_l2 = list(lk[j])[:k-2]\n",
    "            if tmp_l1==tmp_l2: #if first k-1 elements are equal\n",
    "                retList.append(lk[i] | lk[j]) #set union\n",
    "#     print(\"Created C set\")\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(data, threshold=0.01*10000, break_early = False):\n",
    "    \"\"\"\n",
    "    Main function for the algortithm\n",
    "    Break_early is used to stop the algorithm until the sets of 2\n",
    "    \"\"\"\n",
    "    print(\"Start Apriori algorithm\")\n",
    "    c1 = create_prod_list(data)\n",
    "    l1, support_data = create_lk(data, c1, threshold)\n",
    "    L = [l1]\n",
    "    print(\"Value for K:\"+str(1))\n",
    "    print(\"Number of freqent itemsets: \" + str(len(l1)))\n",
    "    k = 2\n",
    "    while (len(L[k-2]) > 0):\n",
    "        print(\"Value for K:\"+str(k))\n",
    "        t = time.time()\n",
    "        ck = create_ck(L[k-2], k)\n",
    "        lk, supK = create_lk(data, ck, threshold)#scan DB to get Lk\n",
    "        support_data.update(supK)\n",
    "        L.append(lk)\n",
    "        k += 1\n",
    "        print(\"Time taken in secs: \"+str((time.time()-t)))\n",
    "        print(\"Number of freqent itemsets: \" + str(len(lk)))\n",
    "        if break_early and k >3:\n",
    "            break\n",
    "    return L, support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Apriori algorithm\n",
      "Product list created!\n",
      "Value for K:1\n",
      "Number of freqent itemsets: 54\n",
      "Value for K:2\n",
      "Time taken in secs: 1.324998378753662\n",
      "Number of freqent itemsets: 137\n",
      "Value for K:3\n",
      "Time taken in secs: 0.4530012607574463\n",
      "Number of freqent itemsets: 127\n",
      "Value for K:4\n",
      "Time taken in secs: 0.10899925231933594\n",
      "Number of freqent itemsets: 37\n",
      "Value for K:5\n",
      "Time taken in secs: 0.008998632431030273\n",
      "Number of freqent itemsets: 5\n",
      "Value for K:6\n",
      "Time taken in secs: 0.0010008811950683594\n",
      "Number of freqent itemsets: 0\n"
     ]
    }
   ],
   "source": [
    "sets, support_info = apriori(basket_data, threshold=0.01*len(basket_data), break_early = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules\n",
    "\n",
    "Rules are defined using confidence as a metric.\n",
    "\n",
    "Confidence:\n",
    "\n",
    "\\begin{equation*}\n",
    "I -> j = \\frac{support({I} + {j})}{support({I})} - \\frac{support({j})}{countOfBaskets}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequent itemsets and the support values are used to write rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conf(itemset, H, support_data, rl, conf=0.7, verbose=False):\n",
    "    \"\"\"Calulates the confidence for each pair\"\"\"\n",
    "    prunedH = []\n",
    "    for conseq in H:\n",
    "        tmp_conf = support_data[itemset]/support_data[itemset-conseq] \n",
    "        if tmp_conf >= conf: \n",
    "            if verbose:\n",
    "                print (itemset-conseq,'-->',conseq,'conf:',tmp_conf)\n",
    "            rl.append((itemset-conseq, conseq, tmp_conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesFromConseq(itemset, H, support_data, rl, conf=0.7, verbose=False):\n",
    "    m = len(H[0])\n",
    "    if (len(itemset) > (m + 1)):    \n",
    "        Hmp1 = create_ck(H, m+1)     \n",
    "        Hmp1 = calculate_conf(itemset, Hmp1, support_data, rl, conf, verbose)\n",
    "        if (len(Hmp1) > 1):    \n",
    "            rulesFromConseq(itemset, Hmp1, support_data, rl, conf, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRules(frequent_itemsets, support_data, conf=0.7, verbose = False): \n",
    "    rule_list = []\n",
    "    for i in range(1, len(frequent_itemsets)):\n",
    "        for itemset in frequent_itemsets[i]:\n",
    "            H1 = [frozenset([item]) for item in itemset]\n",
    "            if (i > 1):\n",
    "                rulesFromConseq(itemset, H1, support_data, rule_list, conf, verbose)\n",
    "            else:\n",
    "                calculate_conf(itemset, H1, support_data, rule_list, conf, verbose)\n",
    "    return rule_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_rule_list = generateRules(sets, support_info, conf=0.7, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Apriori algorithm\n",
      "Product list created!\n",
      "Value for K:1\n",
      "Number of freqent itemsets: 4\n",
      "Value for K:2\n",
      "Time taken in secs: 0.00098419189453125\n",
      "Number of freqent itemsets: 4\n",
      "Value for K:3\n",
      "Time taken in secs: 0.0\n",
      "Number of freqent itemsets: 1\n",
      "Value for K:4\n",
      "Time taken in secs: 0.0\n",
      "Number of freqent itemsets: 0\n"
     ]
    }
   ],
   "source": [
    "tmp_data = [set(ele) for ele in [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]]\n",
    "tmp_sets, tmp_support_info = apriori(tmp_data, threshold=2)\n",
    "\n",
    "tmp_rule_list = generateRules(tmp_sets, tmp_support_info, conf=0.7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
